{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "from PIL import Image\n",
    "from prettytable import PrettyTable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"Trained_on_FairFace_Testing\"\n",
    "model = \"Trained_on_AWIB_FairFace_balanced\"\n",
    "\n",
    "save_path = F\"Z://RR/Final/report_work/results/{model}/FairFace_tests\"\n",
    "df_path = F\"Z://RR/Final/report_work/results/{model}/FairFace_tests/FairFace_results.csv\"\n",
    "\n",
    "FairFace_df = pd.read_csv(df_path)\n",
    "single_mode = False\n",
    "\n",
    "if not single_mode:\n",
    "    save_path = F\"Z://RR/Final/report_work/results/{model}/FairFace_tests_comparitive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(true,predicted,folder,field):\n",
    "\t\n",
    "\taccuracy = metrics.accuracy_score(true,predicted)\n",
    "\tbalanced_accuracy = metrics.balanced_accuracy_score(true,predicted)\n",
    " \n",
    "##########################################################---Accuracy---######################################################################\n",
    "\t\n",
    "\tprecision_macro = metrics.precision_score(true,predicted,average='macro')\n",
    "\tprecision_weighted = metrics.precision_score(true,predicted,average='weighted')\n",
    " \n",
    "##########################################################---Precision--######################################################################\n",
    "\t\n",
    "\trecall_macro = metrics.recall_score(true,predicted,average='macro')\n",
    "\trecall_weighted = metrics.recall_score(true,predicted,average='weighted')\n",
    " \n",
    "##########################################################---Recall--######################################################################\n",
    " \n",
    "\tf1_macro = metrics.f1_score(true,predicted,average='macro')\n",
    "\tf1_weighted = metrics.f1_score(true,predicted,average='weighted')\n",
    " \n",
    "##########################################################---F1 Score---######################################################################\n",
    "\n",
    "\tcohen_kappa = metrics.cohen_kappa_score(true,predicted)\n",
    "\tmatt_corr = metrics.matthews_corrcoef(true,predicted)\n",
    " \n",
    "\n",
    "\tfield_names = ['Metric', 'Result']\n",
    "\t\n",
    "\t# with open(os.path.expanduser(csv_results_output_path),'a') as csvfile:\n",
    "\t# \twriter = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "\t\n",
    "\t# \twriter.writerow({'Metric':F\"accuracy {field}\",'Result':accuracy})\n",
    "\t# \twriter.writerow({'Metric':F\"balanced_accuracy {field}\",'Result':balanced_accuracy})\n",
    "  \n",
    "\t# \twriter.writerow({'Metric':F\"precision_macro {field}\",'Result':precision_macro})\n",
    "\t# \twriter.writerow({'Metric':F\"precision_weighted {field}\",'Result':precision_weighted})\n",
    "  \n",
    "\t# \twriter.writerow({'Metric':F\"recall_macro {field}\",'Result':recall_macro})\n",
    "\t# \twriter.writerow({'Metric':F\"recall_weighted {field}\",'Result':recall_weighted})\n",
    "  \n",
    "\t# \twriter.writerow({'Metric':F\"f1_macro {field}\",'Result':f1_macro})\n",
    "\t# \twriter.writerow({'Metric':F\"f1_weighted {field}\",'Result':f1_weighted})\n",
    "  \n",
    "\t# \twriter.writerow({'Metric':F\"cohen_kappa {field}\",'Result':cohen_kappa})\n",
    "\n",
    "  \n",
    "##########################################################---Writing to CSV File--######################################################################\n",
    "  \n",
    "  \n",
    "\ttable = PrettyTable()\n",
    "\ttable.field_names = field_names\n",
    " \n",
    "\ttable.add_row([F\"{field} accuracy\",accuracy])\n",
    "\ttable.add_row([F\"{field} balanced_accuracy\",balanced_accuracy])\n",
    "\ttable.add_row(['_'*26,'_'*26])\n",
    " \n",
    "\ttable.add_row([F\"{field} precision_macro\",precision_macro])\n",
    "\ttable.add_row([F\"{field} precision_weighted\",precision_weighted])\n",
    "\ttable.add_row(['_'*26,'_'*26])\n",
    " \n",
    "\ttable.add_row([F\"{field} recall_macro\",recall_macro])\n",
    "\ttable.add_row([F\"{field} recall_weighted\",recall_weighted])\n",
    "\ttable.add_row(['_'*26,'_'*26])\n",
    " \n",
    "\ttable.add_row([F\"{field} f1_macro\",f1_macro])\n",
    "\ttable.add_row([F\"{field} f1_weighted\",f1_weighted])\n",
    "\ttable.add_row(['_'*26,'_'*26])\n",
    " \n",
    "\ttable.add_row([F\"{field} cohen_kappa\",cohen_kappa])\n",
    "\ttable.add_row([F\"{field} matthews corrcoef\",matt_corr])\n",
    " \n",
    "\t\n",
    "\tfor f in field_names:\n",
    "\t\ttable.align[f] = \"l\"\n",
    " \n",
    " \n",
    "\t# print(F\"Results for {field}\\n\",table)\n",
    "\tstring_table = table.get_string();\n",
    "\tif not os.path.exists(os.path.join(save_path,folder)):\n",
    "\t\tos.mkdir(os.path.join(save_path,folder))\n",
    "\tfull_save_path = os.path.join(os.path.join(save_path,folder),F\"{field}.txt\")\n",
    "\ttxt_file = open(full_save_path,'w+')\n",
    "\ttxt_file.write(string_table)\n",
    "\ttxt_file.close()\n",
    "\t\n",
    "\tfull_save_path = os.path.join(os.path.join(save_path,folder),F\"{field}.csv\")\n",
    " \n",
    "\twith open(full_save_path, 'w',newline='') as f_output:\n",
    "\t\twriter = csv.DictWriter(f_output, fieldnames=field_names)\n",
    "  \n",
    "\t\tfor i,row in enumerate(table):\n",
    "\t\t\trow_dict = {}\n",
    "\t\t\trow.border = False\n",
    "\t\t\trow.header = False\n",
    "\t\t\tfor f in field_names:\n",
    "\t\t\t\ttemp = row.get_string(fields = [f]).strip()\n",
    "\t\t\t\tif temp[0] != '_':\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t# f_output.write({f:temp})\n",
    "\t\t\t\t\trow_dict[f] = temp\n",
    "\t\t\tif len(row_dict) > 0:\n",
    "\t\t\t\twriter.writerow(row_dict)\n",
    " \n",
    "##########################################################---Displaying table of data--######################################################################\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_true,overall_predicted = FairFace_df['true'],FairFace_df['prediction']\n",
    "if single_mode:  write_results(overall_true,overall_predicted,\"Overall\",\"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def file_results_set(folder,heading,file_name,predictions_arr):\n",
    "\ttable = PrettyTable()\n",
    "\theading.insert(0,'Metric')\n",
    "\t# table.field_names = heading\n",
    " \n",
    "\ttable.add_column('Metric',['accuracy','balanced_accuracy','precision_macro','precision_weighted','recall_macro','recall_weighted','f1_macro','f1_weighted','cohen_kappa','matt_corr'])\n",
    "\t\n",
    "\tfor item in predictions_arr:\n",
    "\t\ttrue = item[0]\n",
    "\t\tpredicted = item[1]\n",
    "\t\tname = item[2]\n",
    "  \n",
    "\t\trounding_pos = 3\n",
    "  \n",
    "\t\taccuracy = round(metrics.accuracy_score(true,predicted),rounding_pos)\n",
    "\n",
    "\t\tbalanced_accuracy = round(metrics.balanced_accuracy_score(true,predicted),rounding_pos)\n",
    "\t\n",
    "\t##########################################################---Accuracy---######################################################################\n",
    "\t\t\n",
    "\t\tprecision_macro = round(metrics.precision_score(true,predicted,average='macro'),rounding_pos)\n",
    "\t\tprecision_weighted = round(metrics.precision_score(true,predicted,average='weighted'),rounding_pos)\n",
    "\t\n",
    "\t##########################################################---Precision--######################################################################\n",
    "\t\t\n",
    "\t\trecall_macro = round(metrics.recall_score(true,predicted,average='macro'),rounding_pos)\n",
    "\t\trecall_weighted = round(metrics.recall_score(true,predicted,average='weighted'),rounding_pos)\n",
    "\t\n",
    "\t##########################################################---Recall--######################################################################\n",
    "\t\n",
    "\t\tf1_macro = round(metrics.f1_score(true,predicted,average='macro'),rounding_pos)\n",
    "\t\tf1_weighted = round(metrics.f1_score(true,predicted,average='weighted'),rounding_pos)\n",
    "\t\n",
    "\t##########################################################---F1 Score---######################################################################\n",
    "\n",
    "\t\tcohen_kappa = round(metrics.cohen_kappa_score(true,predicted),rounding_pos)\n",
    "\t\tmatt_corr = round(metrics.matthews_corrcoef(true,predicted),rounding_pos)\n",
    "\n",
    "\t\ttable.add_column(name,[accuracy,balanced_accuracy,precision_macro,precision_weighted,recall_macro,recall_weighted,f1_macro,f1_weighted,cohen_kappa,matt_corr])\n",
    "\n",
    "\tfor f in heading:\n",
    "\t\ttable.align[f] = \"l\"\n",
    "\t\n",
    "\tstring_table = table.get_string();\n",
    "\tif not os.path.exists(os.path.join(save_path,folder)):\n",
    "\t\tos.mkdir(os.path.join(save_path,folder))\n",
    "  \n",
    "\tfull_save_path = os.path.join(os.path.join(save_path,folder),F\"{file_name}.txt\")\n",
    "\ttxt_file = open(full_save_path,'w+')\n",
    "\ttxt_file.write(string_table)\n",
    "\ttxt_file.close()\n",
    "\t\n",
    "\tfull_save_path = os.path.join(os.path.join(save_path,folder),F\"{file_name}.csv\")\n",
    " \n",
    "\twith open(full_save_path, 'w+',newline='') as f_output:\n",
    "\t\twriter = csv.DictWriter(f_output, fieldnames=table.field_names)\n",
    "\t\trow_dict = {}\n",
    "  \n",
    "\t\tfor f in table.field_names:\n",
    "\t\t\trow_dict[f] = f\n",
    "   \n",
    "\t\twriter.writerow(row_dict)\n",
    "  \n",
    "\t\tfor i,row in enumerate(table):\n",
    "\t\t\trow_dict = {}\n",
    "\t\t\trow.border = False\n",
    "\t\t\trow.header = False\n",
    "\t\t\tfor f in table.field_names:\n",
    "\t\t\t\ttemp = row.get_string(fields = [f]).strip()\n",
    "\t\t\t\tif temp[0] != '_':\n",
    "\t\t\t\t\n",
    "\t\t\t\t\t# f_output.write({f:temp})\n",
    "\t\t\t\t\trow_dict[f] = temp\n",
    "\t\t\tif len(row_dict) > 0:\n",
    "\t\t\t\twriter.writerow(row_dict)\n",
    " \n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def csv_results_set(folder,heading,file_name,predictions_arr):\n",
    "\n",
    "# \theading.insert(0,'Metric')\n",
    "# \tif not os.path.exists(os.path.join(save_path,folder)):\n",
    "# \t\tos.mkdir(os.path.join(save_path,folder))\n",
    "\n",
    "# \tfull_save_path = os.path.join(os.path.join(save_path,folder),F\"{file_name}.csv\")\n",
    "\t\n",
    "# \twith open(full_save_path,'w+') as csvfile:\n",
    "# \t\twriter = csv.DictWriter(csvfile, fieldnames=heading)\n",
    "  \n",
    "# \trow_list = ['accuracy']\n",
    "# \tfor item in predictions_arr:\n",
    "# \t\ttrue = item[0]\n",
    "# \t\tpredicted = item[1]\n",
    "# \t\tname = item[2]\n",
    "# \t\tprint(name)\n",
    "  \n",
    "# \t\taccuracy = round(metrics.accuracy_score(true,predicted),5)\n",
    "# \t\trow_list.append(accuracy)\n",
    "# \trows = zip(row_list)\n",
    "  \n",
    "# \twith open(full_save_path,'a') as csvfile:\n",
    "# \t\twriter = csv.writer(csvfile)\n",
    "# \t\tfor row in rows:\n",
    "# \t\t\twriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arr = []\n",
    "predictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\n",
    "ethnicities = FairFace_df.race.value_counts().reset_index(name = \"count\")['index']\n",
    "\n",
    "for eth in ethnicities:\n",
    "\teth_df = FairFace_df[FairFace_df['race'] == eth]\n",
    "\ttrue,predicted = eth_df['true'],eth_df['prediction']\n",
    "\tpredictions_arr.append((true,predicted,eth))\n",
    "\t\n",
    "\tif single_mode: write_results(true,predicted,'Ethnicity',eth)\n",
    " \n",
    "if not single_mode:\n",
    "    file_results_set('Ethnicity',ethnicities.to_list(),'Ethnicity',predictions_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arr = []\n",
    "predictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\n",
    "age_groups = FairFace_df.age.value_counts().reset_index(name = \"count\")['index'].sort_values().reset_index(drop=True)\n",
    "\n",
    "for ag in age_groups:\n",
    "\tage_df = FairFace_df[FairFace_df['age'] == ag]\n",
    "\ttrue,predicted = age_df['true'],age_df['prediction']\n",
    "\tpredictions_arr.append((true,predicted,ag))\n",
    " \n",
    "\tif single_mode: write_results(true,predicted,'Age',ag)\n",
    " \n",
    "if not single_mode: file_results_set('Age',age_groups.to_list(),'Age',predictions_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_arr = []\n",
    "predictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\n",
    "for g in ['Male','Female']: #0 is Male, 1 is Female\n",
    "\tgender_df = FairFace_df[FairFace_df['gender'] == g]\n",
    "\ttrue,predicted = gender_df['true'],gender_df['prediction']\n",
    "\tif single_mode:  write_results(true,predicted,'Gender',g) \n",
    "\tpredictions_arr.append((true,predicted,g))\n",
    " \n",
    "if not single_mode: file_results_set('Gender',['Male','Female'],'Gender',predictions_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eth in ethnicities:\n",
    "\tpredictions_arr = []\n",
    "\tpredictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\tfor ag in age_groups:\n",
    "\t\ttemp_df = FairFace_df[(FairFace_df['race'] == eth) & (FairFace_df['age'] == ag)]\n",
    "\t\ttrue,predicted = temp_df['true'],temp_df['prediction']\n",
    "\t\tif single_mode:  write_results(true,predicted,'Ethncity Age',F'{eth} {ag}')\n",
    "\t\t\n",
    "\t\tpredictions_arr.append((true,predicted,ag))\n",
    "  \n",
    "\tif not single_mode: file_results_set(F'Ethnicity Age',age_groups.to_list(),F'Eth Age {eth}',predictions_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for eth in ethnicities:\n",
    "\tpredictions_arr = []\n",
    "\tpredictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\tfor g in ['Male','Female']: #0 is Male, 1 is Female\n",
    "\t\ttemp_df = FairFace_df[(FairFace_df['race'] == eth) & (FairFace_df['gender'] == g)]\n",
    "\t\ttrue,predicted = temp_df['true'],temp_df['prediction']\n",
    "\t\tif single_mode: write_results(true,predicted,'Ethncity Gender',F'{eth} {g}') \n",
    "\n",
    "\t\tpredictions_arr.append((true,predicted,g))\n",
    "\tif not single_mode: file_results_set(F'Ethnicity Gender',['Male','Female'],F'Eth Gender {eth}',predictions_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eth in ethnicities:\n",
    "\tfor ag in age_groups:\n",
    "\t\tpredictions_arr = []\n",
    "\t\tpredictions_arr.append((overall_true,overall_predicted,'Overall'))\n",
    "\t\tfor g in ['Male','Female']: #0 is Male, 1 is Female\n",
    "\t\t\ttemp_df = FairFace_df[(FairFace_df['race'] == eth) & (FairFace_df['gender'] == g) & (FairFace_df['age'] == ag)]\n",
    "\t\t\ttrue,predicted = temp_df['true'],temp_df['prediction']\n",
    "\t\t\tif single_mode: write_results(true,predicted,'Ethncity Age Gender',F'{eth} {ag} {g}')\n",
    "\t\t\tpredictions_arr.append((true,predicted,g))\n",
    "   \n",
    "\t\tif not single_mode: file_results_set(F'Ethnicity Age Gender',['Male','Female'],F'Eth Age {ag} Gender {eth}',predictions_arr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01b0da322a7df2b881bf69dce4c75684d5ac75b853286a49a713693279c2c23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
